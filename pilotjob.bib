@inproceedings{pilotreview-2013,
 author = {Shantenu Jha, et\,al.},
 title = {A Fresh Perspective on Pilot-Jobs},
 booktitle = {Technical Report Rutgers University},
 year = {2013},
 address = {NJ, USA},
} 

@article{Walker:2007:PAC:1285840.1285848,
 author = {Walker, Edward and Gardner, Jeffrey P. and Litvin, Vladimir and Turner, Evan L.},
 title = {Personal adaptive clusters as containers for scientific jobs},
 journal = {Cluster Computing},
 issue_date = {September 2007},
 volume = {10},
 number = {3},
 month = sep,
 year = {2007},
 issn = {1386-7857},
 pages = {339--350},
 numpages = {12},
 url = {http://dx.doi.org/10.1007/s10586-007-0028-5},
 doi = {10.1007/s10586-007-0028-5},
 acmid = {1285848},
 publisher = {Kluwer Academic Publishers},
 address = {Hingham, MA, USA},
 keywords = {Cooperative systems, Distributed computing, Resource management},
} 

@misc{mysge,
title={MySGE},
howpublished={\url{http://www.nersc.gov/users/analytics-and-visualization/data-analysis-and-mining/mysge/}}

}

@article{1742-6596-119-6-062012,
  author={S Bagnasco and L Betev and P Buncic and F Carminati and C Cirstoiu and C Grigoras and A Hayrapetyan and A Harutyunyan and A J
Peters and P Saiz},
  title={AliEn: ALICE environment on the GRID},
  journal={Journal of Physics: Conference Series},
  volume={119},
  number={6},
  pages={062012},
  url={http://stacks.iop.org/1742-6596/119/i=6/a=062012},
  year={2008},
  abstract={Starting from mid-2008, the ALICE detector at CERN LHC will collect data at a rate of 4PB per year. ALICE will use exclusively distributed Grid resources to store, process and analyse this data. The top-level management of the Grid resources is done through the AliEn (ALICE Environment) system, which is in continuous development since year 2000. AliEn presents several original solutions, which have shown their viability in a number of large exercises of increasing complexity called Data Challenges. This paper describes the AliEn architecture: Job Management, Data Management and UI. The current status of AliEn will be illustrated, as well as the performance of the system during the data challenges. The paper also describes the future AliEn development roadmap.}
}

@inproceedings{Ahn:2008:ITR:1444448.1445115,
 author = {Ahn, Sunil and Kim, Namgyu and Lee, Seehoon and Hwang, Soonwook and Nam, Dukyun and Koblitz, Birger and Breton, Vincent and Han, Sangyong},
 title = {Improvement of Task Retrieval Performance Using AMGA in a Large-Scale Virtual Screening},
 booktitle = {Proceedings of the 2008 Fourth International Conference on Networked Computing and Advanced Information Management - Volume 01},
 series = {NCM '08},
 year = {2008},
 isbn = {978-0-7695-3322-3-01},
 pages = {456--463},
 numpages = {8},
 url = {http://dx.doi.org/10.1109/NCM.2008.201},
 doi = {10.1109/NCM.2008.201},
 acmid = {1445115},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {WISDOM, AMGA, task retrieval, performance, scalability},
} 

@inproceedings {workqueue-pyhpc2011,
author = "Peter Bui and Dinesh Rajan and Badi Abdul-Wahid and Jesus Izaguirre and Douglas Thain",
title = "{Work Queue + Python: A Framework For Scalable Scientific Ensemble Applications}",
booktitle = "{Workshop on Python for High Performance and Scientific Computing (PyHPC) at the ACM/IEEE International Conference for High Performance Computing, Networking, Storage, and Analysis (Supercomputing) }",
year = 2011,
cclpaperid = "95"
}

@INPROCEEDINGS{wisdom,
   author = {{Bui}, T.~Q. and {Doan}, T.~T. and {Nguyen}, H.~T. and {Pham}, Q.~M. and 
	{Nguyen}, S.~V. and {Breton}, V. and {Pham}, L.~Q. and {Le}, H.~M. and 
	{Nguyen}, H.~Q. and {Medernach}, E.},
    title = "{On the Performance Enhancement of WISDOM Production Environment}",
booktitle = {Proceedings of The International Symposium on Grids and Clouds (ICGC 2012). 26 February-2 March. Taipei, Taiwan. Published online at <A href=''http://pos.sissa.it/cgi-bin/reader/conf.cgi?confid=153''>http://pos.sissa.it/cgi-bin/reader/conf.cgi?confid=153</A>, id.1},
     year = 2012,
      eid = {1},
   adsurl = {http://adsabs.harvard.edu/abs/2012isgc.sympE...1B},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@inproceedings{gwpilot-egi,
  author={Antonio J Rubio-Montero and Eduardo Huedo and Rafael Mayo-Garcia},
  title={GWpilot: a Personal Pilot System},
  booktitle={Proceedings of EGI Forum},
url={\url{https://indico.egi.eu/indico/contributionDisplay.py/pdf?contribId=18&sessionId=46&confId=1019}},
  year={2012},
  abstract={}
}

@inproceedings{Pilot-Data-2013,
 author    = {Andre Luckow and
                Mark Santcroos and
                Ole Weidner and
                Ashley Zebrowski and
                Shantenu Jha},
   title     = {Pilot-Data: An Abstraction for Distributed Data},
   journal   = {CoRR},
   volume    = {abs/1301.6228},
   year      = {2013},
   ee        = {http://arxiv.org/abs/1301.6228},
   bibsource = {DBLP, http://dblp.uni-trier.de}
}
  	
@article{1742-6596-331-7-072069,
  author={Xin Zhao and John Hover and Tomasz Wlodek and Torre Wenaus and Jaime Frey and Todd Tannenbaum and Miron Livny and the ATLAS
Collaboration},
  title={PanDA Pilot Submission using Condor-G: Experience and Improvements},
  journal={Journal of Physics: Conference Series},
  volume={331},
  number={7},
  pages={072069},
  url={http://stacks.iop.org/1742-6596/331/i=7/a=072069},
  year={2011},
  abstract={PanDA (Production and Distributed Analysis) is the workload management system of the ATLAS experiment, used to run managed production and user analysis jobs on the grid. As a late-binding, pilot-based system, the maintenance of a smooth and steady stream of pilot jobs to all grid sites is critical for PanDA operation. The ATLAS Computing Facility (ACF) at BNL, as the ATLAS Tier1 center in the US, operates the pilot submission systems for the US. This is done using the PanDA "AutoPilot" scheduler component which submits pilot jobs via Condor-G, a grid job scheduling system developed at the University of Wisconsin-Madison. In this paper, we discuss the operation and performance of the Condor-G pilot submission at BNL, with emphasis on the challenges and issues encountered in the real grid production environment. With the close collaboration of Condor and PanDA teams, the scalability and stability of the overall system has been greatly improved over the last year. We review improvements made to Condor-G resulting from this collaboration, including isolation of site-based issues by running a separate Gridmanager for each remote site, introduction of the 'Nonessential' job attribute to allow Condor to optimize its behavior for the specific character of pilot jobs, better understanding and handling of the Gridmonitor process, as well as better scheduling in the PanDA pilot scheduler component. We will also cover the monitoring of the health of the system.}
}


@inproceedings{Silberstein:2009:GEB:1654059.1654071,
 author = {Silberstein, Mark and Sharov, Artyom and Geiger, Dan and Schuster, Assaf},
 title = {GridBot: execution of bags of tasks in multiple grids},
 booktitle = {Proceedings of the Conference on High Performance Computing Networking, Storage and Analysis},
 series = {SC '09},
 year = {2009},
 isbn = {978-1-60558-744-8},
 location = {Portland, Oregon},
 pages = {11:1--11:12},
 articleno = {11},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1654059.1654071},
 doi = {10.1145/1654059.1654071},
 acmid = {1654071},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@inproceedings{Rynge:2011:EUG:2116259.2116599,
 author = {Rynge, Mats and Juve, Gideon and Mehta, Gaurang and Deelman, Ewa and Larson, Krista and Holzman, Burt and Sfiligoi, Igor and Wurthwein, Frank and Berriman, G. Bruce and Callaghan, Scott},
 title = {Experiences Using GlideinWMS and the Corral Frontend across Cyberinfrastructures},
 booktitle = {Proceedings of the 2011 IEEE Seventh International Conference on eScience},
 series = {ESCIENCE '11},
 year = {2011},
 isbn = {978-0-7695-4597-4},
 pages = {311--318},
 numpages = {8},
 url = {http://dx.doi.org/10.1109/eScience.2011.50},
 doi = {10.1109/eScience.2011.50},
 acmid = {2116599},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {Condor, glidein, GlideinWMS, Periodograms, CyberShake, Workflow, Corral, OSG, TeraGrid},
}

  	
@article{1742-6596-119-6-062044,
  author={I Sfiligoi},
  title={GlideinWMS—a generic pilot-based workload management system},
  journal={Journal of Physics: Conference Series},
  volume={119},
  number={6},
  pages={062044},
  url={http://stacks.iop.org/1742-6596/119/i=6/a=062044},
  year={2008},
  abstract={The Grid resources are distributed among hundreds of independent Grid sites, requiring a higher level Workload Management System (WMS) to be used efficiently. Pilot jobs have been used for this purpose by many communities, bringing increased reliability, global fair share and just in time resource matching. glideinWMS is a WMS based on the Condor glidein concept, i.e. a regular Condor pool, with the Condor daemons (startds) being started by pilot jobs, and real jobs being vanilla, standard or MPI universe jobs. The glideinWMS is composed of a set of Glidein Factories, handling the submission of pilot jobs to a set of Grid sites, and a set of VO Frontends, requesting pilot submission based on the status of user jobs. This paper contains the structural overview of glideinWMS as well as a detailed description of the current implementation and the current scalability limits.}
}

@techreport{Casanova:1995:NNS:898848,
 author = {Casanova, H. and Dongarra, J.},
 title = {NetSolve: A Network Server for Solving Computational Science Problems},
 year = {1995},
 source = {http://www.ncstrl.org:8900/ncstrl/servlet/search?formname=detail\&id=oai%3Ancstrlh%3Autk_cs%3Ancstrl.utk_cs%2F%2FUT-CS-95-313},
 publisher = {University of Tennessee},
 address = {Knoxville, TN, USA},
}

@inproceedings{Lampson:1983:HCS:800217.806614,
 author = {Lampson, Butler W.},
 title = {Hints for computer system design},
 booktitle = {Proceedings of the ninth ACM symposium on Operating systems principles},
 series = {SOSP '83},
 year = {1983},
 isbn = {0-89791-115-6},
 location = {Bretton Woods, New Hampshire, United States},
 pages = {33--48},
 numpages = {16},
 doi = {10.1145/800217.806614},
 acmid = {806614},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@misc{copilot-tr, note = {"Co-Pilot: The Distributed Job Execution
                  Framework", Predrag Buncic, Artem Harutyunyan, Technical Report, 
Portable Analysis Environment using Virtualization Technology (WP9), CERN}}

@misc{lgi, 
author="{NIKHEF}",
title={LGI Pilotjob Framework},
howpublished={\url{http://wiki.nikhef.nl/grid/LGI_Pilotjob_Framework}}
}



@article{Berman:2003:ACG:766629.766632,
 author = {Berman, Francine and Wolski, Richard and Casanova, Henri and Cirne, Walfredo and Dail, Holly and Faerman, Marcio and Figueira, Silvia and Hayes, Jim and Obertelli, Graziano and Schopf, Jennifer and Shao, Gary and Smallen, Shava and Spring, Neil and Su, Alan and Zagorodnov, Dmitrii},
 title = {Adaptive Computing on the Grid Using AppLeS},
 journal = {IEEE Trans. Parallel Distrib. Syst.},
 issue_date = {April 2003},
 volume = {14},
 number = {4},
 month = apr,
 year = {2003},
 issn = {1045-9219},
 pages = {369--382},
 numpages = {14},
 url = {http://dx.doi.org/10.1109/TPDS.2003.1195409},
 doi = {10.1109/TPDS.2003.1195409},
 acmid = {766632},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
 keywords = {Scheduling, parallel and distributed computing, heterogeneous computing, grid computing.},
}

@misc{venusc-generic-worker,
author={Christian Geuer-Pollmann},
title={The Venus C Generic Worker},
howpublished={SICS Cloud Day},
year="2011"
}

@inproceedings{5171374,
	Author = {Sfiligoi, I. and Bradley, D.C. and Holzman, B. and Mhashilkar, P. and Padhi, S. and Wurthwein, F.},
	Booktitle = {Computer Science and Information Engineering},
	Date-Added = {2012-07-14 09:11:44 +0000},
	Date-Modified = {2012-07-14 09:11:44 +0000},
	Doi = {10.1109/CSIE.2009.950},
	Keywords = {glideinWMS;grid computing;grid middleware;grid resources;job scheduling complexity;open science grid;grid computing;middleware;scheduling;},
	Pages = {428 -432},
	Title = {The Pilot Way to Grid Resources Using GlideinWMS},
	Year = {2009},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/CSIE.2009.950}}

@misc{glideinwms,
	Date-Added = {2012-07-14 09:04:56 +0000},
	Date-Modified = {2012-07-14 09:08:36 +0000},
	Howpublished = {\url{http://www.uscms.org/SoftwareComputing/Grid/WMS/glideinWMS/doc.prd/index.html}},
	Title = {GlideinWMS - The Glidein-based Workflow Management System},
	Year = {2012}}


@inproceedings{pstar-2012,
	Author = {Andre Luckow and Mark Santcroos and Ole Weider and Andre Merzky and Sharath Maddineni and Shantenu Jha},
	Booktitle = {Proceedings of The International ACM Symposium on High-Performance Parallel and Distributed Computing},
	Date-Added = {2011-09-04 16:13:49 +0000},
	Date-Modified = {2011-09-04 16:14:42 +0000},
	Title = {Towards a Common Model for Pilot-Jobs},
	Year = {2012}}
	


@article{1742-6596-78-1-012057,
	Abstract = {The Open Science Grid (OSG) provides a distributed facility where the Consortium members provide guaranteed and opportunistic access to shared computing and storage resources. OSG provides support for and evolution of the infrastructure through activities that cover operations, security, software, troubleshooting, addition of new capabilities, and support for existing and engagement with new communities. The OSG SciDAC-2 project provides specific activities to manage and evolve the distributed infrastructure and support it's use. The innovative aspects of the project are the maintenance and performance of a collaborative (shared & common) petascale national facility over tens of autonomous computing sites, for many hundreds of users, transferring terabytes of data a day, executing tens of thousands of jobs a day, and providing robust and usable resources for scientific groups of all types and sizes. More information can be found at the OSG web site: www.opensciencegrid.org.},
	Author = {Ruth Pordes {\it et al}},
	Journal = {Journal of Physics: Conference Series},
	Number = {1},
	Pages = {012057},
	Title = {The open science grid},
	Volume = {78},
	Year = {2007},
	Bdsk-Url-1 = {http://stacks.iop.org/1742-6596/78/i=1/a=012057}}

@article{1742-6596-78-1-012057-long,
	Abstract = {The Open Science Grid (OSG) provides a distributed facility where the Consortium members provide guaranteed and opportunistic access to shared computing and storage resources. OSG provides support for and evolution of the infrastructure through activities that cover operations, security, software, troubleshooting, addition of new capabilities, and support for existing and engagement with new communities. The OSG SciDAC-2 project provides specific activities to manage and evolve the distributed infrastructure and support it's use. The innovative aspects of the project are the maintenance and performance of a collaborative (shared & common) petascale national facility over tens of autonomous computing sites, for many hundreds of users, transferring terabytes of data a day, executing tens of thousands of jobs a day, and providing robust and usable resources for scientific groups of all types and sizes. More information can be found at the OSG web site: www.opensciencegrid.org.},
	Author = {Ruth Pordes and Don Petravick and Bill Kramer and Doug Olson and Miron Livny and Alain Roy and Paul Avery and Kent Blackburn and Torre Wenaus and Frank W{\"u}rthwein and Ian Foster and Rob Gardner and Mike Wilde and Alan Blatecky and John McGee and Rob Quick},
	Journal = {Journal of Physics: Conference Series},
	Number = {1},
	Pages = {012057},
	Title = {The open science grid},
	Url = {http://stacks.iop.org/1742-6596/78/i=1/a=012057},
	Volume = {78},
	Year = {2007},
	Bdsk-Url-1 = {http://stacks.iop.org/1742-6596/78/i=1/a=012057}}

@manual{OMG-CORBA303:2004,
	Key = {OMG},
	Keywords = {2004, corba, omg},
	Month = {M},
	Organization = {Object Management Group},
	Posted-At = {2006-10-04 15:54:44},
	Priority = {0},
	Title = {{Common Object Request Broker Architecture: Core Specification}},
	Year = {2004}}

@article{1742-6596-219-6-062049,
	Abstract = {DIRAC, the LHCb community Grid solution, has pioneered the use of pilot jobs in the Grid. Pilot Jobs provide a homogeneous interface to an heterogeneous set of computing resources. At the same time, Pilot Jobs allow to delay the scheduling decision to the last moment, thus taking into account the precise running conditions at the resource and last moment requests to the system. The DIRAC Workload Management System provides one single scheduling mechanism for jobs with very different profiles. To achieve an overall optimisation, it organizes pending jobs in task queues, both for individual users and production activities. Task queues are created with jobs having similar requirements. Following the VO policy a priority is assigned to each task queue. Pilot submission and subsequent job matching are based on these priorities following a statistical approach.},
	Author = {Adrian Casajus and Ricardo Graciani and Stuart Paterson and Andrei Tsaregorodtsev},
	Journal = {Journal of Physics: Conference Series},
	Number = {6},
	Pages = {062049},
	Title = {DIRAC pilot framework and the DIRAC Workload Management System},
	Volume = {219},
	Year = {2010},
	Bdsk-Url-1 = {http://stacks.iop.org/1742-6596/219/i=6/a=062049}}

@inproceedings{dare-tg11-gateways,
	Author = {Joohyun Kim and Sharath Maddineni and Shantenu Jha},
	Booktitle = {Proceedings of TeraGrid'11 Extreme Discovery},
	Title = {Building Gateways for Life-Science Applications using the Distributed Adaptive Runtime Environment (DARE) Framework},
	Year = 2011}

@phdthesis{diane-thesis,
	Author = {Jakub Tomasz Moscicki},
	Date-Added = {2011-04-22 19:11:25 +0200},
	Date-Modified = {2011-04-22 19:15:12 +0200},
	School = {University of Amsterdam},
	Title = {Understanding and Mastering Dynamics in Computing Grids: Processing Moldable Tasks with User-Level Overlay},
        url = {http://dare.uva.nl/record/1/333467},
	Year = {2011}}

@inproceedings{Doraimani:2008:FGS:1383422.1383429,
	Acmid = {1383429},
	Address = {New York, NY, USA},
	Author = {Doraimani, Shyamala and Iamnitchi, Adriana},
	Booktitle = {Proceedings of the 17th international symposium on High performance distributed computing},
	Doi = {http://doi.acm.org/10.1145/1383422.1383429},
	Isbn = {978-1-59593-997-5},
	Keywords = {caching, data management, file grouping, job scheduling, science grids, trace analysis},
	Location = {Boston, MA, USA},
	Numpages = {12},
	Pages = {153--164},
	Publisher = {ACM},
	Series = {HPDC '08},
	Title = {File grouping for scientific data management: lessons from experimenting with real traces},
	Url = {http://doi.acm.org/10.1145/1383422.1383429},
	Year = {2008},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1383422.1383429}}

@inproceedings{ramakrishnan2011,
	Author = {Zacharia Fadika and Elif Dede and Madhusudhan Govindaraju and Lavanya Ramakrishnan},
	Booktitle = {submitted to The International Conference for High Performance Computing, Networking, Storage, and Analysis},
	Title = {Benchmarking MapReduce Implementations for Application Usage Scenarios},
	Year = {2011}}

@inproceedings{weissman2011,
	Author = {Michael Cardosa and Chenyu Wang and Anshuman Nangia and Abhishek Chandra and Jon Weissman},
	Title = {Exploring MapReduce Efficiency with Highly-Distributed Data},
	Year = {2011}}

@inproceeding{gray2000,
	Author = {Jim Gray, Prashant Shenoy},
	Howpublished = {\url{http://research.microsoft.com/en-us/um/people/gray/papers/ms_tr_99_100_rules_of_thumb_in_data_engineering.pdf}},
	Title = {Rules of Thumb in Data Engineering},
	Year = {2000}}

@book{hey2009,
	Abstract = {Increasingly, scientific breakthroughs will be powered by advanced computing computingapabilities that help researchers manipulate and explore massive datasets.

The speed at which any given scientific discipline advances will depend on how well its researchers collaborate with one another, and with technologists, in areas of eScience such as databases, workflow management, visualization, and cloud computing technologies.

In The Fourth Paradigm: Data-Intensive Scientific Discovery, the collection of essays expands on the vision of pioneering computer scientist Jim Gray for a new, fourth paradigm of discovery based on data-intensive science and offers insights into how it can be fully realized.},
	Added-At = {2010-02-16T09:54:37.000+0100},
	Address = {USA},
	Biburl = {http://www.bibsonomy.org/bibtex/28b203c0313656b6ced70c14c86a4c42a/acka47},
	Date-Added = {2011-09-04 20:56:39 +0000},
	Date-Modified = {2011-09-04 20:57:08 +0000},
	Editor = {Tony Hey and Stewart Tansley and Kristin Tolle},
	Interhash = {296450016ca8a5f8ab16ae4d92d1fc15},
	Intrahash = {8b203c0313656b6ced70c14c86a4c42a},
	Keywords = {research scholarly_communication science},
	Publisher = {Microsoft Research},
	Timestamp = {2010-02-16T09:54:37.000+0100},
	Title = {The Fourth Paradigm: Data-Intensive Scientific Discovery},
	Year = 2009,
	Bdsk-Url-1 = {http://research.microsoft.com/en-us/collaboration/fourthparadigm/}}

@article{10.1109/MIC.2011.64,
	Address = {Los Alamitos, CA, USA},
	Author = {Ian Foster},
	Doi = {http://doi.ieeecomputersociety.org/10.1109/MIC.2011.64},
	Issn = {1089-7801},
	Journal = {IEEE Internet Computing},
	Pages = {70-73},
	Publisher = {IEEE Computer Society},
	Title = {Globus Online: Accelerating and Democratizing Science through Cloud-Based Services},
	Volume = {15},
	Year = {2011},
	Bdsk-Url-1 = {http://doi.ieeecomputersociety.org/10.1109/MIC.2011.64}}

@misc{webhdfs,
	Howpublished = {\url{http://hadoop.apache.org/common/docs/r1.0.0/webhdfs.html}},
	Key = {webhdfs},
	Title = {{WebHDFS REST API}},
	Year = 2012}

@inproceedings{Moscicki:908910,
	Abstract = { Distributed analysis environment (DIANE) is the result of R D in CERN IT Division focused on interfacing semi-interactive parallel applications with distributed GRID technology. DIANE provides a master-worker workflow management layer above low-level GRID services. DIANE is application and language-neutral. Component-container architecture and component adapters provide flexibility necessary to fulfill the diverse requirements of distributed applications. Physical transport layer assures interoperability with existing middleware frameworks based on Web services. Several distributed simulations based on Geant 4 were deployed and tested in real-life scenarios with DIANE.},
	Author = {Moscicki, J.T.},
	Booktitle = {Nuclear Science Symposium Conference Record, 2003 IEEE},
	Doi = {10.1109/NSSMIC.2003.1352187},
	Issn = {1082-3654},
	Keywords = {CERN IT Division; GRID-enabled physics data simulation; Geant 4; Web services; component adapters; component-container architecture; distributed analysis environment; interoperability; master-worker workflow management layer; middleware frameworks; physical transport layer; physics data analysis; semi-interactive parallel applications; Internet; data analysis; grid computing; middleware; physics computing; workflow management software;},
	Pages = {1617 - 1620},
	Title = {DIANE - distributed analysis environment for GRID-enabled simulation and analysis of physics data},
	Volume = {3},
	Year = {2003},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/NSSMIC.2003.1352187}}

@misc{bigjob_web,
	Howpublished = {\url{http://saga-project.github.com/BigJob/}},
	Key = {SAGA BigJob},
	Title = {{SAGA BigJob}},
	Year = {2012}}

@misc{pilot_api,
	Author = {{Pilot API}},
	Howpublished = {\url{http://radicalpilot.readthedocs.org/en/stable/apidoc.html}},
	Key = {Pilot},
	Year = 2015}

@article{condor-g,
	Author = {Frey, J. and Tannenbaum, T. and Livny, M. and Foster, I. and Tuecke, S.},
	Citeulike-Article-Id = {291860},
	Date-Added = {2008-02-28 10:08:47 -0600},
	Date-Modified = {2008-06-30 19:47:43 +0200},
	Doi = {10.1023/A:1015617019423},
	Journal = {Cluster Computing},
	Keywords = {grid, scheduling},
	Month = {July},
	Number = {3},
	Pages = {237--246},
	Priority = {2},
	Title = {{Condor-G: A Computation Management Agent for Multi-Institutional Grids}},
	Volume = {5},
	Year = {2002}
	}

@article{condor-g-short,
	Author = {Frey, J. and Tannenbaum, T. and Livny, M. and Foster, I. and Tuecke, S.},
	Citeulike-Article-Id = {291860},
	Date-Added = {2008-02-28 10:08:47 -0600},
	Date-Modified = {2008-06-30 19:47:43 +0200},
	Doi = {10.1023/A:1015617019423},
	Journal = {Cluster Computing},
	Keywords = {grid, scheduling},
	Month = {July},
	Number = {3},
	Pages = {237--246},
	Priority = {2},
	Title = {{Condor-G: A Computation Management Agent for Multi-Institutional Grids}},
	Volume = {5},
	Year = {2002},
	Bdsk-Url-1 = {http://dx.doi.org/10.1023/A:1015617019423}}

@inproceedings{1362680,
	Address = {New York, NY, USA},
	Author = {Ioan Raicu and Yong Zhao and Catalin Dumitrescu and Ian Foster and Mike Wilde},
	Booktitle = {SC '07: Proceedings of the 2007 ACM/IEEE conference on Supercomputing},
	Date-Added = {2008-08-09 21:04:33 +0200},
	Date-Modified = {2008-08-09 21:04:53 +0200},
	Doi = {http://doi.acm.org/10.1145/1362622.1362680},
	Isbn = {978-1-59593-764-3},
	Location = {Reno, Nevada},
	Pages = {1--12},
	Publisher = {ACM},
	Title = {{Falkon: A Fast and Light-Weight TasK ExecutiON Framework}},
	Year = {2007},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1362622.1362680}}

@incollection{karajan,
  title={Java CoG kit workflow},
  author={Von Laszewski, Gregor and Hategan, Mihael and Kodeboyina, Deepti},
  booktitle={Workflows for e-Science},
  pages={340--356},
  year={2007},
  publisher={Springer}
}

@inproceedings{raicu2008accelerating,
  title={Accelerating large-scale data exploration through data diffusion},
  author={Raicu, Ioan and Zhao, Yong and Foster, Ian T and Szalay, Alex},
  booktitle={Proceedings of the 2008 international workshop on Data-aware distributed computing},
  pages={9--18},
  year={2008},
  organization={ACM}
}

@article{Wilde2011,
	Abstract = {Scientists, engineers, and statisticians must execute domain-specific application programs many times on large collections of file-based data. This activity requires complex orchestration and data management as data is passed to, from, and among application invocations. Distributed and parallel computing resources can accelerate such processing, but their use further increases programming complexity. The Swift parallel scripting language reduces these complexities by making file system structures accessible via language constructs and by allowing ordinary application programs to be composed into powerful parallel scripts that can efficiently utilize parallel and distributed resources. We present Swift's implicitly parallel and deterministic programming model, which applies external applications to file collections using a functional style that abstracts and simplifies distributed parallel execution.},
	Author = {Michael Wilde and Mihael Hategan and Justin M. Wozniak and Ben Clifford and Daniel S. Katz and Ian Foster},
	Doi = {10.1016/j.parco.2011.05.005},
	Issn = {0167-8191},
	Journal = {Parallel Computing},
	Keywords = {Dataflow},
	Number = {9},
	Pages = {633--652},
	Title = {Swift: A language for distributed parallel scripting},
	Volume = {37},
	Year = {2011},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0167819111000524},
	Bdsk-Url-2 = {http://dx.doi.org/10.1016/j.parco.2011.05.005}}

@misc{coasters,
	Howpublished = {\url{http://wiki.cogkit.org/wiki/Coasters}},
	Key = {coasters},
	Title = {Coasters},
	Year = 2009}

@misc{topos,
	Howpublished = {\url{https://grid.sara.nl/wiki/index.php/Using_the_Grid/ToPoS}},
	Key = {Topos},
	Title = {ToPoS - A Token Pool Server for Pilot Jobs},
	Year = 2011}

@inproceedings{1652061,
	Author = {Walker, E. and Gardner, J.P. and Litvin, V. and Turner, E.L.},
	Booktitle = {Challenges of Large Applications in Distributed Environments, 2006 IEEE},
	Doi = {10.1109/CLADE.2006.1652061},
	Keywords = {Condor;NSF TeraGrid;Sun Grid Engine cluster;cooperative system;distributed computing environment;personal adaptive cluster;resource management;scientific job management;grid computing;resource allocation;workstation clusters;},
	Month = {0-0},
	Pages = {95-103},
	Title = {Creating personal adaptive clusters for managing scientific jobs in a distributed computing environment},
	Year = {2006},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/CLADE.2006.1652061}}

@article{10.1109/HPC.2000.846563,
	Address = {Los Alamitos, CA, USA},
	Author = {Rajkumar Buyya and David Abramson and Jonathan Giddy},
	Doi = {http://doi.ieeecomputersociety.org/10.1109/HPC.2000.846563},
	Isbn = {0-7695-0589-2},
	Journal = {International Conference on High-Performance Computing in the Asia-Pacific Region},
	Pages = {283-289},
	Publisher = {IEEE Computer Society},
	Title = {Nimrod/G: An Architecture for a Resource Management and Scheduling System in a Global Computational Grid},
	Volume = {1},
	Year = {2000},
	Bdsk-Url-1 = {http://doi.ieeecomputersociety.org/10.1109/HPC.2000.846563}}

@article{1742-6596-219-6-062041,
	Abstract = {The Panda Workload Management System is designed around the concept of the Pilot Job -- a "smart wrapper" for the payload executable that can probe the environment on the remote worker node before pulling down the payload from the server and executing it. Such design allows for improved logging and monitoring capabilities as well as flexibility in Workload Management. In the Grid environment (such as the Open Science Grid), Panda Pilot Jobs are submitted to remote sites via mechanisms that ultimately rely on Condor-G. As our experience has shown, in cases where a large number of Panda jobs are simultaneously routed to a particular remote site, the increased load on the head node of the cluster, which is caused by the Pilot Job submission, may lead to overall lack of scalability. We have developed a Condor-inspired solution to this problem, which is using the schedd-based glidein, whose mission is to redirect pilots to the native batch system. Once a glidein schedd is installed and running, it can be utilized exactly the same way as local schedds and therefore, from the user's perspective, Pilots thus submitted are quite similar to jobs submitted to the local Condor pool.},
	Author = {Po-Hsiang Chiu and Maxim Potekhin},
	Journal = {Journal of Physics: Conference Series},
	Number = {6},
	Pages = {062041},
	Title = {Pilot factory -- a Condor-based system for scalable Pilot Job generation in the Panda WMS framework},
	Volume = {219},
	Year = {2010},
	Bdsk-Url-1 = {http://stacks.iop.org/1742-6596/219/i=6/a=062041}}

@misc{egi,
	Author = {{EGI}},
	Howpublished = {\url{http://www.egi.eu/}},
	Key = {egi},
	Year = {2012}}

@article{bfast2009,
	Author = {Homer, N. and Merriman, B. and Nelson, S. F.},
	Journal = {PLoS One},
	Number = {11},
	Pages = {e7767},
	Title = {{BFAST : An alignment tool for large scale genome resequencing}},
	Volume = {4},
	Year = {2009}}

@misc{saga_rm,
	Author = {Andre Merzky},
	Booktitle = {OGF Draft},
	Howpublished = {\url{https://svn.cct.lsu.edu/repos/saga-ogf/trunk/documents/saga-package-resource/}},
	Title = {{SAGA Resource Management API (DRAFT)}},
	Year = 2011}

@misc{saga_advert,
	Author = {Andre Merzky},
	Howpublished = {OGF Document Series 177, \url{http://www.gridforum.org/documents/GFD.177.pdf}},
	Title = {{SAGA API Extension: Advert API}},
	Year = 2011}

@misc{redis,
	Author = {{Redis}},
	Howpublished = {\url{http://redis.io/}},
	Key = {Redis},
	Year = 2012}

@misc{zmq,
	Author = {{ZeroMQ}},
	Howpublished = {\url{http://www.zeromq.org/}},
	Key = {zmq},
	Year = 2012}

@misc{fg,
	Howpublished = {\url{https://portal.futuregrid.org/}},
	Key = {FutureGrid},
	Title = {{FutureGrid: An Experimental, High-Performance Grid Test-bed}},
	Year = 2012}

@misc{xsede,
	Howpublished = {\url{https://www.xsede.org/}},
	Key = {XSEDE},
	Title = {{XSEDE: Extreme Science and Engineering Discovery Environment}},
	Year = 2012}

@article{Gelernter:1985:GCL:2363.2433,
	Acmid = {2433},
	Address = {New York, NY, USA},
	Author = {Gelernter, David},
	Doi = {http://doi.acm.org/10.1145/2363.2433},
	Issn = {0164-0925},
	Issue = {1},
	Journal = {ACM Trans. Program. Lang. Syst.},
	Month = {January},
	Numpages = {33},
	Pages = {80--112},
	Publisher = {ACM},
	Title = {Generative communication in Linda},
	Url = {http://doi.acm.org/10.1145/2363.2433},
	Volume = {7},
	Year = {1985},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2363.2433}}

@misc{loni,
	Author = {{LONI}},
	Howpublished = {\url{http://www.loni.org}},
	Key = {loni},
	Year = {2011}}

@article{Moscicki20092303,
	Abstract = {In this paper, we present the computational task-management tool Ganga, which allows for the specification, submission, bookkeeping and post-processing of computational tasks on a wide set of distributed resources. Ganga has been developed to solve a problem increasingly common in scientific projects, which is that researchers must regularly switch between different processing systems, each with its own command set, to complete their computational tasks. Ganga provides a homogeneous environment for processing data on heterogeneous resources. We give examples from High Energy Physics, demonstrating how an analysis can be developed on a local system and then transparently moved to a Grid system for processing of all available data. Ganga has an API that can be used via an interactive interface, in scripts, or through a GUI. Specific knowledge about types of tasks or computational resources is provided at run-time through a plugin system, making new developments easy to integrate. We give an overview of the Ganga architecture, give examples of current use, and demonstrate how Ganga can be used in many different areas of science.
Program summary
Program title: Ganga

Catalogue identifier: AEEN_v1_0

Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEEN_v1_0.html

Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland

Licensing provisions: GPL

No. of lines in distributed program, including test data, etc.: 224 590

No. of bytes in distributed program, including test data, etc.: 14 365 315

Distribution format: tar.gz

Programming language: Python

Computer: personal computers, laptops

Operating system: Linux/Unix

RAM: 1 MB

Classification: 6.2, 6.5

Nature of problem: Management of computational tasks for scientific applications on heterogenous distributed systems, including local, batch farms, opportunistic clusters and Grids.

Solution method: High-level job management interface, including command line, scripting and GUI components.

Restrictions: Access to the distributed resources depends on the installed, 3rd party software such as batch system client or Grid user interface.},
	Author = {J.T. Moscicki {\it et al}},
	Doi = {10.1016/j.cpc.2009.06.016},
	Issn = {0010-4655},
	Journal = {Computer Physics Communications},
	Keywords = {Application configuration},
	Number = {11},
	Pages = {2303 - 2316},
	Title = {Ganga: A tool for computational-task management and easy access to Grid resources},
	Volume = {180},
	Year = {2009},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0010465509001970},
	Bdsk-Url-2 = {http://dx.doi.org/10.1016/j.cpc.2009.06.016}}

@article{Moscicki20092303-long,
	Abstract = {In this paper, we present the computational task-management tool Ganga, which allows for the specification, submission, bookkeeping and post-processing of computational tasks on a wide set of distributed resources. Ganga has been developed to solve a problem increasingly common in scientific projects, which is that researchers must regularly switch between different processing systems, each with its own command set, to complete their computational tasks. Ganga provides a homogeneous environment for processing data on heterogeneous resources. We give examples from High Energy Physics, demonstrating how an analysis can be developed on a local system and then transparently moved to a Grid system for processing of all available data. Ganga has an API that can be used via an interactive interface, in scripts, or through a GUI. Specific knowledge about types of tasks or computational resources is provided at run-time through a plugin system, making new developments easy to integrate. We give an overview of the Ganga architecture, give examples of current use, and demonstrate how Ganga can be used in many different areas of science.
Program summary
Program title: Ganga

Catalogue identifier: AEEN_v1_0

Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEEN_v1_0.html

Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland

Licensing provisions: GPL

No. of lines in distributed program, including test data, etc.: 224 590

No. of bytes in distributed program, including test data, etc.: 14 365 315

Distribution format: tar.gz

Programming language: Python

Computer: personal computers, laptops

Operating system: Linux/Unix

RAM: 1 MB

Classification: 6.2, 6.5

Nature of problem: Management of computational tasks for scientific applications on heterogenous distributed systems, including local, batch farms, opportunistic clusters and Grids.

Solution method: High-level job management interface, including command line, scripting and GUI components.

Restrictions: Access to the distributed resources depends on the installed, 3rd party software such as batch system client or Grid user interface.},
	Author = {J.T. Moscicki and F. Brochu and J. Ebke and U. Egede and J. Elmsheuser and K. Harrison and R.W.L. Jones and H.C. Lee and D. Liko and A. Maier and A. Muraru and G.N. Patrick and K. Pajchel and W. Reece and B.H. Samset and M.W. Slater and A. Soroko and C.L. Tan and D.C. van der Ster and M. Williams},
	Doi = {10.1016/j.cpc.2009.06.016},
	Issn = {0010-4655},
	Journal = {Computer Physics Communications},
	Keywords = {Application configuration},
	Number = {11},
	Pages = {2303 - 2316},
	Title = {Ganga: A tool for computational-task management and easy access to Grid resources},
	Url = {http://www.sciencedirect.com/science/article/pii/S0010465509001970},
	Volume = {180},
	Year = {2009},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0010465509001970},
	Bdsk-Url-2 = {http://dx.doi.org/10.1016/j.cpc.2009.06.016}}

@misc{leaky_abstractions,
	Author = {{Joel Spolsky}},
	Note = {\url{http://www.joelonsoftware.com/articles/LeakyAbstractions.html}},
	Title = {{The Law of Leaky Abstractions}}}

%http://iopscience.iop.org/1742-6596/396/3/032116?fromSearchPage=true
@article{bosco,
  author={Derek Weitzel and Dan Fraser and Brian Bockelman and David Swanson},
  title={Campus Grids: Bringing Additional Computational Resources to HEP Researchers},
  journal={Journal of Physics: Conference Series},
  volume={396},
  number={3},
  pages={032116},
  url={http://stacks.iop.org/1742-6596/396/i=3/a=032116},
  year={2012},
  abstract={It is common at research institutions to maintain multiple clusters that represent different owners or generations of hardware, or that fulfill different needs and policies. Many of these clusters are consistently under utilized while researchers on campus could greatly benefit from these unused capabilities. By leveraging principles from the Open Science Grid it is now possible to utilize these resources by forming a lightweight campus grid. The campus grids framework enables jobs that are submitted to one cluster to overflow, when necessary, to other clusters within the campus using whatever authentication mechanisms are available on campus. This framework is currently being used on several campuses to run HEP and other science jobs. Further, the framework has in some cases been expanded beyond the campus boundary by bridging campus grids into a regional grid, and can even be used to integrate resources from a national cyberinfrastructure such as the Open Science Grid. This paper will highlight 18 months of operational experiences creating campus grids in the US, and the different campus configurations that have successfully utilized the campus grid infrastructure.}
}

% IEEE http://ieeexplore.ieee.org.proxy.libraries.rutgers.edu/xpl/articleDetails.jsp?tp=&arnumber=6266981&contentType=Conference+Publications&searchField%3DSearch_All%26queryText%3DPerformance+improvements+for+the+neoclassical+transport+calculation+on+Grid+by+means+of+pilot+jobs
@INPROCEEDINGS{gwpilot,
author={Rubio-Montero, A.J. and Castejon, F. and Huedo, E. and Rodriguez-Pascual, M. and Mayo-Garcia, R.},
booktitle={High Performance Computing and Simulation (HPCS), 2012 International Conference on}, title={Performance improvements for the neoclassical transport calculation on Grid by means of pilot jobs},
year={2012},
month={july},
volume={},
number={},
pages={609 -615},
keywords={Databases;Dispatching;Europe;Middleware;Plasmas;Standards;Suspensions;Tokamak devices;grid computing;middleware;physics computing;plasma confinement;plasma transport processes;DRMAA-enabled DKEsG version;drift kinetic equation solver;fusion device;generic pilot-job platform;grid infrastructure;monoenergetic coefficient;neoclassical transport;plasmas;standard grid middleware;stellarator;tokamaks;transport coefficient;Drift Kinetic Equation solver;TJ-II;late-binding;neoclassical transport;pilot jobs;},
doi={10.1109/HPCSim.2012.6266981},
ISSN={},}

@inproceedings{masterworker,
 author = {Heymann, Elisa and Senar, Miquel A. and Luque, Emilio and Livny, Miron},
 title = {Adaptive Scheduling for Master-Worker Applications on the Computational Grid},
 booktitle = {Proceedings of the First IEEE/ACM International Workshop on Grid Computing},
 series = {GRID '00},
 year = {2000},
 isbn = {3-540-41403-7},
 pages = {214--227},
 numpages = {14},
 url = {http://dl.acm.org/citation.cfm?id=645440.652833},
 acmid = {652833},
 publisher = {Springer-Verlag},
 address = {London, UK, UK},
} 

@INPROCEEDINGS{Goux00anenabling,
    author = {Jean-pierre Goux and Sanjeev Kulkarni and Jeff Linderoth and Michael Yoder},
    title = {An Enabling Framework for Master-Worker Applications on the Computational Grid},
    booktitle = {Cluster Computing},
    year = {2000},
    pages = {43--50},
    publisher = {Society Press}
}

@inproceedings{nilsson2011atlas,
  title={The ATLAS PanDA Pilot in Operation},
  author={Nilsson, P and Caballero, J and De, K and Maeno, T and Stradling, A and Wenaus, T and others},
  booktitle={Journal of Physics: Conference Series},
  volume={331, 6},
  pages={062040},
  year={2011},
  organization={IOP Publishing}
}

@INPROCEEDINGS{Pinchak02practicalheterogeneous,
    author = {Christopher Pinchak and Paul Lu and Mark Goldenberg},
    title = {Practical Heterogeneous Placeholder Scheduling In Overlay Metacomputers: Early Experiences},
    booktitle = {In Proc. 8th Workshop on Job Scheduling Strategies for Parallel Processing (JSSPP},
    year = {2002},
    pages = {85--105},
    publisher = {Springer Verlag}
}

@inproceedings{Singh:2008:WTC:1341811.1341822,
 author = {Singh, Gurmeet and Su, Mei-Hui and Vahi, Karan and Deelman, Ewa and Berriman, Bruce and Good, John and Katz, Daniel S. and Mehta, Gaurang},
 title = {Workflow task clustering for best effort systems with Pegasus},
 booktitle = {Proceedings of the 15th ACM Mardi Gras conference: From lightweight mash-ups to lambda grids: Understanding the spectrum of distributed computing requirements, applications, tools, infrastructures, interoperability, and the incremental adoption of key capabilities},
 series = {MG '08},
 year = {2008},
 isbn = {978-1-59593-835-0},
 location = {Baton Rouge, Louisiana},
 pages = {9:1--9:8},
 articleno = {9},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1341811.1341822},
 doi = {10.1145/1341811.1341822},
 acmid = {1341822},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {best effort systems, queue wait time, task clustering, workflow clustering},
} 

@article{katramatos2001jobqueue,
  title={JobQueue: A computational grid-wide queuing system},
  author={Katramatos, Dimitrios and Humphrey, Marty and Grimshaw, Andrew and Chapin, Steve},
  journal={Grid Computing—GRID 2001},
  pages={99--110},
  year={2001},
  publisher={Springer}
}

@inproceedings{Anderson:2004:BSP:1032646.1033223,
 author = {Anderson, David P.},
 title = {BOINC: A System for Public-Resource Computing and Storage},
 booktitle = {Proceedings of the 5th IEEE/ACM International Workshop on Grid Computing},
 series = {GRID '04},
 year = {2004},
 isbn = {0-7695-2256-4},
 pages = {4--10},
 numpages = {7},
 url = {http://dx.doi.org/10.1109/GRID.2004.14},
 doi = {10.1109/GRID.2004.14},
 acmid = {1033223},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@article{ghs,
 author = {Wu, Ming and Sun, Xian-He},
 title = {Grid harvest service: a performance system of grid computing},
 journal = {J. Parallel Distrib. Comput.},
 issue_date = {October 2006},
 volume = {66},
 number = {10},
 month = oct,
 year = {2006},
 issn = {0743-7315},
 pages = {1322--1337},
 numpages = {16},
 url = {http://dx.doi.org/10.1016/j.jdpc.2006.05.008},
 doi = {10.1016/j.jdpc.2006.05.008},
 acmid = {1217682},
 publisher = {Academic Press, Inc.},
 address = {Orlando, FL, USA},
 keywords = {grid computing, performance modeling, performance prediction and measurement, resources sharing, task scheduling},
} 

@inproceedings{maeno_pd2p:_2012,
	title = {{PD2P:} {PanDA} Dynamic Data Placement for {ATLAS}},
	volume = {396},
	shorttitle = {{PD2P}},
	url = {http://iopscience.iop.org/1742-6596/396/3/032070},
	urldate = {2013-04-26},
	booktitle = {Journal of Physics: Conference Series},
	author = {Maeno, Tadashi and De, K. and Panitkin, S.},
	year = {2012},
	pages = {032070},
	file = {ATL-SOFT-PROC-2012-016.pdf:/home/ashley/.mozilla/firefox/uywjh76x.default/zotero/storage/8HF2GFQV/ATL-SOFT-PROC-2012-016.pdf:application/pdf}
}

@inproceedings{nilsson2012recent,
  title={Recent Improvements in the ATLAS PanDA Pilot},
  author={Nilsson, P and Bejar, J Caballero and Compostella, G and Contreras, C and De, K and Dos Santos, T and Maeno, T and Potekhin, M and Wenaus, T},
  booktitle={Journal of Physics: Conference Series},
  volume={396},
  number={3},
  pages={032080},
  year={2012},
  organization={IOP Publishing}
}

@misc{oar,
	Howpublished = {\url{http://oar.imag.fr/dokuwiki/doku.php}},
	Key = {oar},
	Title = {OAR},
	Year = 2013}
}

@inproceedings{pandapresentation2013-06,
    author  = "Kaushik De",
    title   = "Next Generation Workload Management System for Big Data",
    publisher = "Presented at the BNL HPC Workshop",
    year    = 2013
}

@article{Gehring:1996:mars,
    author = {Gehring, J{\"o}rn and Reinefeld, Alexander},
    title = {{MARS---a framework for minimizing the job execution time in a
    metacomputing environment}},
    journal = {Future Generation Computer Systems},
    year = {1996},
    volume = {12},
    number = {1},
    month = may
}

@article{Belforte:2006:glidecaf,
author = {Belforte, S and Hsu, ISC and Lipeles, E and Norman, M},
title = {{GlideCAF: A Late Binding Approach to the Grid}},
journal = {CHEP06},
year = {2006}
}

@article{Epema:1996:flocking,
author = {Epema, D H J and Livny, M and van Dantzig, R and Evers, X and Pruyne,
J},
title = {{A worldwide flock of Condors: load sharing among workstation
clusters}},
journal = {Future Generation Computer Systems},
year = {1996},
volume = {12},
number = {1},
month = may
}

@article{Saiz:2003:alien,
author = {Saiz, Pablo and Buncic, Predrag and Peters, Andreas J},
title = {{AliEn Resource Brokers}},
year = {2003},
eprint = {cs/0306068v1},
eprinttype = {arxiv},
eprintclass = {cs.DC},
month = jun,
annote = {5 pages, 8 figures, CHEP 03 conference}
}

@inproceedings{Berman:1996:apples,
author = {Berman, Francine D and Wolski, Rich and Figueira, Silvia and Schopf,
Jennifer and Shao, Gary},
title = {{Application-level scheduling on distributed heterogeneous networks}},
booktitle = {Supercomputing '96: Proceedings of the 1996 ACM/IEEE conference on
Supercomputing},
year = {1996},
publisher = {IEEE Computer Society},
month = nov
}

@inproceedings{Shao:2000:masterslave,
author = {Shao, Gary and Berman, Francine and Wolski, Rich},
title = {{Master/Slave Computing on the Grid}},
booktitle = {HCW '00: Proceedings of the 9th Heterogeneous Computing Workshop},
year = {2000},
publisher = { IEEE Computer Society},
month = may,
}

@misc{woltman:2004:gimps,
title={The Great Internet Mersenne Prime Search},
author={Woltman, George and Kurowski, Scott},
journal={h ttp://www. mersenne. org/, 200609 11},
year={2004}
}

@article{Lawton:2000:distributednet,
author = {Lawton, G},
title = {{Distributed net applications create virtual supercomputers}},
journal = {Computer},
year = {2000},
volume = {33},
number = {6},
pages = {16--20}
}

@article{Thain:2005wz,
author = {Thain, Douglas and Tannenbaum, Todd and Livny, Miron},
title = {{Distributed Computing in Practice:The Condor Experience}},
journal = {Concurrency and Computation: Practice {\&} Experience - Grid
Performance},
year = {2005},
volume = {17},
number = {2‐4},
pages = {323--356},
month = feb
}

@inproceedings{thota-xsede13,
  author    = {Abhinav Thota and
               Bernhard Haubold and
               Scott Michael and
               Thomas Doak and
               Sen Xu and
               Robert Henschel},
  title     = {Making campus bridging work for researchers: a case study
               with mlRho},
  booktitle = {XSEDE},
  year      = {2013},
  pages     = {17},
  ee        = {http://doi.acm.org/10.1145/2484762.2484803},
  OPTcrossref  = {DBLP:conf/xsede/2013},
  OPTbibsource = {DBLP, http://dblp.uni-trier.de}
}

@techreport{lhcb2005,
    author = {{The LHCb collaboration}},
    title = {{LHCb computing: Technical Design Report}},
    year = {2005},
    month = jun
}

@article {dpa_surveypaper,
author = {Jha, Shantenu and Cole, Murray and Katz, Daniel S. and Parashar, Manish and Rana, Omer and Weissman, Jon},
title = {Distributed computing practice for large-scale science and engineering applications},
journal = {Concurrency and Computation: Practice and Experience},
volume = {25},
number = {11},
issn = {1532-0634},
url = {http://dx.doi.org/10.1002/cpe.2897},
doi = {10.1002/cpe.2897},
pages = {1559--1585},
year = {2013},
}

@inproceedings{dpagrid2009,
  author      = {Shantenu Jha and Daniel S. Katz and Manish Parashar and Omer 
                Rana and Jon B. Weissman},
  title       = {{Critical Perspectives on Large-Scale Distributed Applications
                and Production Grids (Best Paper Award)}},
  booktitle   = {{The 10th IEEE/ACM Conference on Grid Computing 2009}},
  year        = 2009,
  pages       = {1-8},
  doi         = {10.1109/GRID.2009.5353064},
  bibsource   = {http://dblp.uni-trier.de},
  note         = {\url{http://dx.doi.org/10.1109/GRID.2009.5353064}},
}
<<<<<<< HEAD
=======

>>>>>>> f1f2c1225de3c128f7f57f7fec17ab6c6f32f1ed
